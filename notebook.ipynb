{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winter Wheat BBCH Prediction Model using Growing Degree Days (GDD)\n",
    "\n",
    "This notebook builds a phenological prediction model for winter wheat (*Triticum aestivum*) using:\n",
    "- **DWD Phenology Data**: Historical BBCH stage observations from German weather stations\n",
    "- **DWD Climate Data**: Daily temperature data for calculating Growing Degree Days\n",
    "\n",
    "## BBCH Stages for Winter Wheat\n",
    "The BBCH scale is a standardized phenological development scale:\n",
    "- **BBCH 10**: Emergence (first leaf through soil surface)\n",
    "- **BBCH 21**: Beginning of tillering\n",
    "- **BBCH 31**: Beginning of stem elongation\n",
    "- **BBCH 51**: Beginning of heading\n",
    "- **BBCH 61**: Beginning of flowering\n",
    "- **BBCH 75**: Medium milk ripening\n",
    "- **BBCH 87**: Hard dough (end of ripening)\n",
    "\n",
    "## Growing Degree Days (GDD)\n",
    "GDD accumulates thermal time above a base temperature:\n",
    "$$GDD = \\sum_{i=1}^{n} max(0, T_{mean,i} - T_{base})$$\n",
    "\n",
    "For winter wheat, typical base temperature is **0°C** (some studies use 5°C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "PARAMS = {\n",
    "    # Growing Degree Days parameters\n",
    "    'T_base': 0.0,           # Base temperature for GDD calculation (°C)\n",
    "    'T_upper': 30.0,         # Upper temperature cutoff (°C)\n",
    "    \n",
    "    # Growing season definition\n",
    "    'season_start_month': 1,  # January (winter wheat resumes growth)\n",
    "    'season_start_day': 1,\n",
    "    \n",
    "    # BBCH stages tracked by DWD for winter wheat\n",
    "    'bbch_stages': {\n",
    "        10: 'Emergence',\n",
    "        12: 'Two leaves unfolded',\n",
    "        21: 'Beginning of tillering',\n",
    "        31: 'Beginning of stem elongation',\n",
    "        41: 'Flag leaf sheath extending',\n",
    "        51: 'Beginning of heading',\n",
    "        61: 'Beginning of flowering',\n",
    "        75: 'Medium milk ripening',\n",
    "        87: 'Hard dough'\n",
    "    },\n",
    "    \n",
    "    # Data sources\n",
    "    'phenology_url_hist': 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/phenology/annual_reporters/crops/historical/PH_Jahresmelder_Landwirtschaft_Kulturpflanze_Winterweizen_1925_2023_hist.txt',\n",
    "    'phenology_url_recent': 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/phenology/annual_reporters/crops/recent/PH_Jahresmelder_Landwirtschaft_Winterweizen_akt.txt',\n",
    "    'climate_base_url': 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/',\n",
    "    'stations_url': 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/KL_Tageswerte_Beschreibung_Stationen.txt'\n",
    "}\n",
    "\n",
    "print(\"Parameters defined:\")\n",
    "print(f\"  Base temperature: {PARAMS['T_base']}°C\")\n",
    "print(f\"  Upper temperature cutoff: {PARAMS['T_upper']}°C\")\n",
    "print(f\"  BBCH stages tracked: {list(PARAMS['bbch_stages'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download DWD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data directory relative to notebook location\nDATA_DIR = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data')\nos.makedirs(DATA_DIR, exist_ok=True)\nprint(f\"Data directory: {DATA_DIR}\")\n\ndef download_phenology_data(cache_file=None):\n    \"\"\"Download winter wheat phenology data from DWD with caching.\"\"\"\n    if cache_file is None:\n        cache_file = os.path.join(DATA_DIR, 'phenology_winterwheat.csv')\n    \n    # Check if cached file exists\n    if os.path.exists(cache_file):\n        print(f\"Loading cached phenology data from {cache_file}...\")\n        df = pd.read_csv(cache_file)\n        df.columns = df.columns.str.strip()\n        print(f\"Loaded {len(df)} phenology records from cache\")\n        return df\n    \n    print(\"Downloading phenology data from DWD (this may take a minute)...\")\n    \n    # Download historical data\n    url = PARAMS['phenology_url_hist']\n    response = requests.get(url)\n    \n    if response.status_code == 200:\n        # Parse the data - DWD uses semicolon separator\n        df = pd.read_csv(\n            io.StringIO(response.text),\n            sep=';',\n            encoding='latin-1'\n        )\n        # Strip whitespace from column names (DWD has leading spaces)\n        df.columns = df.columns.str.strip()\n        \n        # Cache the data\n        df.to_csv(cache_file, index=False)\n        print(f\"Downloaded {len(df)} phenology records and cached to {cache_file}\")\n        return df\n    else:\n        print(f\"Failed to download: {response.status_code}\")\n        return None\n\n# Download phenology data\nphenology_df = download_phenology_data()\nif phenology_df is not None:\n    print(f\"\\nColumns: {phenology_df.columns.tolist()}\")\n    phenology_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the phenology data structure\nif phenology_df is not None:\n    # Strip column names in case loaded from old cache\n    phenology_df.columns = phenology_df.columns.str.strip()\n    \n    print(\"Data types:\")\n    print(phenology_df.dtypes)\n    print(f\"\\nShape: {phenology_df.shape}\")\n    print(f\"\\nUnique stations: {phenology_df['Stations_id'].nunique()}\")\n    print(f\"Year range: {phenology_df['Referenzjahr'].min()} - {phenology_df['Referenzjahr'].max()}\")\n    print(f\"\\nUnique phases (BBCH): {sorted(phenology_df['Phase_id'].unique())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_climate_stations():\n",
    "    \"\"\"Download list of climate stations.\"\"\"\n",
    "    print(\"Downloading station list...\")\n",
    "    \n",
    "    url = PARAMS['stations_url']\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse fixed-width format\n",
    "        lines = response.text.split('\\n')\n",
    "        # Skip header lines\n",
    "        data_lines = [l for l in lines[2:] if l.strip()]\n",
    "        \n",
    "        stations = []\n",
    "        for line in data_lines:\n",
    "            if len(line) > 50:\n",
    "                try:\n",
    "                    station_id = line[0:5].strip()\n",
    "                    von_datum = line[6:14].strip()\n",
    "                    bis_datum = line[15:23].strip()\n",
    "                    hoehe = line[24:38].strip()\n",
    "                    lat = line[39:50].strip()\n",
    "                    lon = line[51:60].strip()\n",
    "                    name = line[61:].strip()\n",
    "                    \n",
    "                    stations.append({\n",
    "                        'station_id': int(station_id) if station_id.isdigit() else None,\n",
    "                        'from_date': von_datum,\n",
    "                        'to_date': bis_datum,\n",
    "                        'elevation': float(hoehe) if hoehe else None,\n",
    "                        'lat': float(lat) if lat else None,\n",
    "                        'lon': float(lon) if lon else None,\n",
    "                        'name': name\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        df = pd.DataFrame(stations)\n",
    "        df = df.dropna(subset=['station_id'])\n",
    "        print(f\"Found {len(df)} climate stations\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "climate_stations = download_climate_stations()\n",
    "if climate_stations is not None:\n",
    "    climate_stations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_climate_data(station_id, start_year, end_year):\n    \"\"\"Download daily climate data for a specific station with caching.\"\"\"\n    station_str = str(int(station_id)).zfill(5)\n    cache_dir = os.path.join(DATA_DIR, 'climate')\n    os.makedirs(cache_dir, exist_ok=True)\n    cache_file = os.path.join(cache_dir, f'climate_station_{station_str}.csv')\n    \n    # Check if cached file exists\n    if os.path.exists(cache_file):\n        df = pd.read_csv(cache_file)\n        df.columns = df.columns.str.strip()\n        return df\n    \n    # Try historical data first\n    base_url = PARAMS['climate_base_url']\n    hist_url = f\"{base_url}historical/\"\n    \n    try:\n        response = requests.get(hist_url)\n        if response.status_code == 200:\n            import re\n            pattern = f'tageswerte_KL_{station_str}_.*_hist\\.zip'\n            matches = re.findall(pattern, response.text)\n            \n            if matches:\n                zip_url = f\"{hist_url}{matches[0]}\"\n                zip_response = requests.get(zip_url)\n                \n                if zip_response.status_code == 200:\n                    with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n                        data_file = [f for f in z.namelist() if f.startswith('produkt_klima')]\n                        if data_file:\n                            with z.open(data_file[0]) as f:\n                                df = pd.read_csv(f, sep=';')\n                                df.columns = df.columns.str.strip()\n                                \n                                # Cache the data\n                                df.to_csv(cache_file, index=False)\n                                return df\n    except Exception as e:\n        pass\n    \n    return None\n\n# Test with one station\ntest_station = 433\nprint(f\"Testing climate data download for station {test_station}...\")\ntest_climate = download_climate_data(test_station, 2010, 2020)\nif test_climate is not None:\n    print(f\"Downloaded {len(test_climate)} daily records\")\n    print(f\"Columns: {test_climate.columns.tolist()}\")\n    test_climate.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_phenology_data(df):\n",
    "    \"\"\"Clean and preprocess phenology data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    column_mapping = {\n",
    "        'Stations_id': 'station_id',\n",
    "        'Referenzjahr': 'year',\n",
    "        'Qualitaetsniveau': 'quality',\n",
    "        'Objekt_id': 'object_id',\n",
    "        'Phase_id': 'phase_id',\n",
    "        'Eintrittsdatum': 'entry_date',\n",
    "        'Eintrittsdatum_QB': 'entry_date_qb',\n",
    "        'Jultag': 'doy'  # Day of Year\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
    "    \n",
    "    # Convert entry_date to datetime\n",
    "    if 'entry_date' in df.columns:\n",
    "        df['entry_date'] = pd.to_datetime(df['entry_date'], format='%Y%m%d', errors='coerce')\n",
    "    \n",
    "    # Filter for recent years with good data quality\n",
    "    df = df[df['year'] >= 1990]\n",
    "    \n",
    "    # Remove invalid entries\n",
    "    df = df.dropna(subset=['entry_date', 'doy'])\n",
    "    df = df[df['doy'] > 0]\n",
    "    \n",
    "    print(f\"Preprocessed data: {len(df)} records\")\n",
    "    print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")\n",
    "    print(f\"Unique stations: {df['station_id'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if phenology_df is not None:\n",
    "    phenology_clean = preprocess_phenology_data(phenology_df)\n",
    "    phenology_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze available BBCH phases\n",
    "if phenology_clean is not None:\n",
    "    phase_counts = phenology_clean.groupby('phase_id').size().sort_index()\n",
    "    print(\"Records per BBCH phase:\")\n",
    "    for phase, count in phase_counts.items():\n",
    "        phase_name = PARAMS['bbch_stages'].get(phase, 'Unknown')\n",
    "        print(f\"  Phase {phase} ({phase_name}): {count:,} records\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    phase_counts.plot(kind='bar', ax=ax, color='forestgreen', edgecolor='darkgreen')\n",
    "    ax.set_xlabel('BBCH Phase ID')\n",
    "    ax.set_ylabel('Number of Observations')\n",
    "    ax.set_title('Distribution of BBCH Phase Observations for Winter Wheat')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Growing Degree Days (GDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gdd(temp_mean, t_base=0, t_upper=30):\n",
    "    \"\"\"Calculate Growing Degree Days for a single day.\"\"\"\n",
    "    if pd.isna(temp_mean):\n",
    "        return 0\n",
    "    \n",
    "    # Clip temperature to upper limit\n",
    "    temp_effective = min(temp_mean, t_upper)\n",
    "    \n",
    "    # Calculate GDD (only positive values)\n",
    "    gdd = max(0, temp_effective - t_base)\n",
    "    \n",
    "    return gdd\n",
    "\n",
    "def calculate_cumulative_gdd(climate_df, year, end_doy, t_base=0, t_upper=30):\n",
    "    \"\"\"Calculate cumulative GDD from Jan 1 to a specific day of year.\"\"\"\n",
    "    df = climate_df.copy()\n",
    "    \n",
    "    # Filter for the specific year\n",
    "    df['date'] = pd.to_datetime(df['MESS_DATUM'], format='%Y%m%d', errors='coerce')\n",
    "    df = df[df['date'].dt.year == year]\n",
    "    df['doy'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Filter from Jan 1 to end_doy\n",
    "    df = df[df['doy'] <= end_doy]\n",
    "    \n",
    "    # Temperature column (TMK = daily mean temperature)\n",
    "    if 'TMK' in df.columns:\n",
    "        temp_col = 'TMK'\n",
    "    elif ' TMK' in df.columns:\n",
    "        temp_col = ' TMK'\n",
    "    else:\n",
    "        # Find temperature column\n",
    "        temp_cols = [c for c in df.columns if 'TM' in c.upper()]\n",
    "        temp_col = temp_cols[0] if temp_cols else None\n",
    "    \n",
    "    if temp_col is None:\n",
    "        return None\n",
    "    \n",
    "    # Replace missing values (-999) with NaN\n",
    "    df[temp_col] = df[temp_col].replace(-999, np.nan)\n",
    "    \n",
    "    # Calculate daily GDD\n",
    "    df['gdd'] = df[temp_col].apply(lambda x: calculate_gdd(x, t_base, t_upper))\n",
    "    \n",
    "    # Sum cumulative GDD\n",
    "    cumulative_gdd = df['gdd'].sum()\n",
    "    \n",
    "    return cumulative_gdd\n",
    "\n",
    "print(\"GDD calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset: Match phenology observations with climate data\n",
    "def build_training_dataset(phenology_df, sample_stations=50, years_range=(2000, 2020)):\n",
    "    \"\"\"\n",
    "    Build training dataset by matching phenology observations with GDD.\n",
    "    \"\"\"\n",
    "    print(\"Building training dataset...\")\n",
    "    \n",
    "    # Get unique station-year combinations\n",
    "    station_years = phenology_df[['station_id', 'year']].drop_duplicates()\n",
    "    station_years = station_years[\n",
    "        (station_years['year'] >= years_range[0]) & \n",
    "        (station_years['year'] <= years_range[1])\n",
    "    ]\n",
    "    \n",
    "    # Sample stations for faster processing\n",
    "    unique_stations = station_years['station_id'].unique()\n",
    "    if len(unique_stations) > sample_stations:\n",
    "        np.random.seed(42)\n",
    "        selected_stations = np.random.choice(unique_stations, sample_stations, replace=False)\n",
    "    else:\n",
    "        selected_stations = unique_stations\n",
    "    \n",
    "    print(f\"Processing {len(selected_stations)} stations...\")\n",
    "    \n",
    "    results = []\n",
    "    climate_cache = {}\n",
    "    \n",
    "    for station_id in selected_stations:\n",
    "        # Download climate data for this station (cache it)\n",
    "        if station_id not in climate_cache:\n",
    "            climate_data = download_climate_data(station_id, years_range[0], years_range[1])\n",
    "            climate_cache[station_id] = climate_data\n",
    "        else:\n",
    "            climate_data = climate_cache[station_id]\n",
    "        \n",
    "        if climate_data is None:\n",
    "            continue\n",
    "        \n",
    "        # Get phenology observations for this station\n",
    "        station_pheno = phenology_df[\n",
    "            (phenology_df['station_id'] == station_id) &\n",
    "            (phenology_df['year'] >= years_range[0]) &\n",
    "            (phenology_df['year'] <= years_range[1])\n",
    "        ]\n",
    "        \n",
    "        for _, row in station_pheno.iterrows():\n",
    "            year = row['year']\n",
    "            doy = int(row['doy'])\n",
    "            phase_id = row['phase_id']\n",
    "            \n",
    "            # Calculate cumulative GDD to this date\n",
    "            gdd = calculate_cumulative_gdd(\n",
    "                climate_data, year, doy,\n",
    "                t_base=PARAMS['T_base'],\n",
    "                t_upper=PARAMS['T_upper']\n",
    "            )\n",
    "            \n",
    "            if gdd is not None and gdd > 0:\n",
    "                results.append({\n",
    "                    'station_id': station_id,\n",
    "                    'year': year,\n",
    "                    'phase_id': phase_id,\n",
    "                    'doy': doy,\n",
    "                    'gdd': gdd\n",
    "                })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    print(f\"Built dataset with {len(result_df)} samples\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Build the dataset (this may take a few minutes)\n",
    "print(\"Note: This will download climate data for multiple stations. This may take several minutes.\")\n",
    "training_data = build_training_dataset(phenology_clean, sample_stations=30, years_range=(2010, 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If download is slow, create synthetic training data based on typical GDD values\n",
    "def create_synthetic_training_data():\n",
    "    \"\"\"\n",
    "    Create synthetic training data based on literature GDD thresholds for winter wheat.\n",
    "    This allows the model to be trained even if data download is slow.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Typical GDD requirements for winter wheat BBCH stages (base 0°C)\n",
    "    # These are approximate values from agricultural literature\n",
    "    gdd_thresholds = {\n",
    "        10: (100, 30),    # Emergence: ~100 GDD\n",
    "        12: (180, 40),    # Two leaves: ~180 GDD\n",
    "        21: (300, 50),    # Tillering: ~300 GDD\n",
    "        31: (550, 70),    # Stem elongation: ~550 GDD\n",
    "        41: (750, 80),    # Flag leaf: ~750 GDD\n",
    "        51: (950, 90),    # Heading: ~950 GDD\n",
    "        61: (1100, 100),  # Flowering: ~1100 GDD\n",
    "        75: (1400, 120),  # Milk ripening: ~1400 GDD\n",
    "        87: (1700, 150),  # Hard dough: ~1700 GDD\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    for phase_id, (mean_gdd, std_gdd) in gdd_thresholds.items():\n",
    "        # Generate multiple samples per phase\n",
    "        n_samples = 200\n",
    "        gdds = np.random.normal(mean_gdd, std_gdd, n_samples)\n",
    "        gdds = np.clip(gdds, mean_gdd - 2*std_gdd, mean_gdd + 2*std_gdd)\n",
    "        \n",
    "        # Convert GDD back to approximate DOY (assuming ~15 GDD/day in spring)\n",
    "        doys = gdds / 12 + 60  # Rough approximation\n",
    "        \n",
    "        for gdd, doy in zip(gdds, doys):\n",
    "            data.append({\n",
    "                'station_id': np.random.randint(1, 100),\n",
    "                'year': np.random.randint(2010, 2021),\n",
    "                'phase_id': phase_id,\n",
    "                'doy': int(doy),\n",
    "                'gdd': gdd\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Use real data if available, otherwise use synthetic\n",
    "if training_data is None or len(training_data) < 100:\n",
    "    print(\"Using synthetic training data based on literature GDD values...\")\n",
    "    training_data = create_synthetic_training_data()\n",
    "\n",
    "print(f\"\\nTraining data shape: {training_data.shape}\")\n",
    "print(f\"Samples per phase:\")\n",
    "print(training_data.groupby('phase_id').size())\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GDD distribution per BBCH phase\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: GDD distribution per phase\n",
    "ax1 = axes[0, 0]\n",
    "gdd_by_phase = training_data.groupby('phase_id')['gdd'].agg(['mean', 'std']).reset_index()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(gdd_by_phase)))\n",
    "bars = ax1.bar(gdd_by_phase['phase_id'].astype(str), gdd_by_phase['mean'], \n",
    "               yerr=gdd_by_phase['std'], capsize=5, color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('BBCH Phase')\n",
    "ax1.set_ylabel('Growing Degree Days (°C·days)')\n",
    "ax1.set_title('Mean GDD Required for Each BBCH Phase')\n",
    "\n",
    "# Plot 2: Boxplot of GDD per phase\n",
    "ax2 = axes[0, 1]\n",
    "training_data.boxplot(column='gdd', by='phase_id', ax=ax2)\n",
    "ax2.set_xlabel('BBCH Phase')\n",
    "ax2.set_ylabel('Growing Degree Days (°C·days)')\n",
    "ax2.set_title('GDD Distribution per BBCH Phase')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Plot 3: GDD vs DOY\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(training_data['doy'], training_data['gdd'], \n",
    "                      c=training_data['phase_id'], cmap='viridis', alpha=0.6)\n",
    "ax3.set_xlabel('Day of Year')\n",
    "ax3.set_ylabel('Growing Degree Days (°C·days)')\n",
    "ax3.set_title('GDD vs Day of Year')\n",
    "plt.colorbar(scatter, ax=ax3, label='BBCH Phase')\n",
    "\n",
    "# Plot 4: Phase progression\n",
    "ax4 = axes[1, 1]\n",
    "mean_gdd = training_data.groupby('phase_id')['gdd'].mean()\n",
    "ax4.plot(mean_gdd.index, mean_gdd.values, 'o-', linewidth=2, markersize=10, color='forestgreen')\n",
    "for phase, gdd in mean_gdd.items():\n",
    "    phase_name = PARAMS['bbch_stages'].get(phase, str(phase))\n",
    "    ax4.annotate(phase_name, (phase, gdd), textcoords=\"offset points\", \n",
    "                 xytext=(5, 5), fontsize=8, rotation=45)\n",
    "ax4.set_xlabel('BBCH Phase')\n",
    "ax4.set_ylabel('Mean GDD (°C·days)')\n",
    "ax4.set_title('Phenological Progression (GDD Accumulation)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bbch_gdd_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGDD Statistics per BBCH Phase:\")\n",
    "stats = training_data.groupby('phase_id')['gdd'].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train BBCH Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Model 1: Predict BBCH phase from GDD\n",
    "X = training_data[['gdd']].values\n",
    "y = training_data['phase_id'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: GDD (cumulative growing degree days)\")\n",
    "print(f\"Target: BBCH phase ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'CV_MAE': cv_mae,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE:  {mae:.2f} BBCH units\")\n",
    "    print(f\"  RMSE: {rmse:.2f} BBCH units\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    print(f\"  CV MAE: {cv_mae:.2f} BBCH units\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Model comparison (bar chart)\n",
    "ax1 = axes[0, 0]\n",
    "model_names = list(results.keys())\n",
    "maes = [results[m]['MAE'] for m in model_names]\n",
    "r2s = [results[m]['R2'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, maes, width, label='MAE', color='steelblue')\n",
    "ax1.set_ylabel('MAE (BBCH units)')\n",
    "ax1.set_title('Model Comparison: Mean Absolute Error')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: R² comparison\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['green' if r > 0.9 else 'orange' if r > 0.8 else 'red' for r in r2s]\n",
    "bars2 = ax2.bar(model_names, r2s, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('R² Score')\n",
    "ax2.set_title('Model Comparison: R² Score')\n",
    "ax2.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='Excellent (0.9)')\n",
    "ax2.axhline(y=0.8, color='orange', linestyle='--', alpha=0.5, label='Good (0.8)')\n",
    "ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Actual vs Predicted (best model)\n",
    "best_model_name = min(results.keys(), key=lambda m: results[m]['MAE'])\n",
    "best_model = results[best_model_name]\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(y_test, best_model['predictions'], alpha=0.5, color='steelblue')\n",
    "ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "ax3.set_xlabel('Actual BBCH Phase')\n",
    "ax3.set_ylabel('Predicted BBCH Phase')\n",
    "ax3.set_title(f'Actual vs Predicted ({best_model_name})')\n",
    "\n",
    "# Plot 4: Prediction curve\n",
    "ax4 = axes[1, 1]\n",
    "gdd_range = np.linspace(0, 2000, 200).reshape(-1, 1)\n",
    "for name, result in results.items():\n",
    "    pred = result['model'].predict(gdd_range)\n",
    "    ax4.plot(gdd_range, pred, label=name, linewidth=2)\n",
    "\n",
    "ax4.scatter(X_test, y_test, alpha=0.3, color='gray', s=10, label='Test data')\n",
    "ax4.set_xlabel('Growing Degree Days (°C·days)')\n",
    "ax4.set_ylabel('Predicted BBCH Phase')\n",
    "ax4.set_title('Model Predictions vs GDD')\n",
    "ax4.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} (MAE: {best_model['MAE']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Analysis and GDD Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GDD thresholds for each BBCH stage\n",
    "print(\"Estimated GDD Thresholds for BBCH Stages (Winter Wheat)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gdd_thresholds = training_data.groupby('phase_id')['gdd'].agg(['mean', 'std', 'min', 'max'])\n",
    "gdd_thresholds.columns = ['Mean GDD', 'Std GDD', 'Min GDD', 'Max GDD']\n",
    "\n",
    "for phase_id, row in gdd_thresholds.iterrows():\n",
    "    phase_name = PARAMS['bbch_stages'].get(phase_id, f'Phase {phase_id}')\n",
    "    print(f\"\\nBBCH {phase_id} ({phase_name}):\")\n",
    "    print(f\"  Mean GDD: {row['Mean GDD']:.0f} °C·days\")\n",
    "    print(f\"  Std Dev:  {row['Std GDD']:.0f} °C·days\")\n",
    "    print(f\"  Range:    {row['Min GDD']:.0f} - {row['Max GDD']:.0f} °C·days\")\n",
    "\n",
    "gdd_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function\n",
    "best_model_obj = results[best_model_name]['model']\n",
    "\n",
    "def predict_bbch(gdd, model=best_model_obj):\n",
    "    \"\"\"\n",
    "    Predict BBCH stage from accumulated Growing Degree Days.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdd : float\n",
    "        Accumulated Growing Degree Days (base 0°C)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Predicted BBCH stage\n",
    "    str : Stage description\n",
    "    \"\"\"\n",
    "    pred = model.predict([[gdd]])[0]\n",
    "    \n",
    "    # Round to nearest valid BBCH stage\n",
    "    valid_stages = list(PARAMS['bbch_stages'].keys())\n",
    "    closest_stage = min(valid_stages, key=lambda x: abs(x - pred))\n",
    "    \n",
    "    return closest_stage, PARAMS['bbch_stages'][closest_stage]\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nExample Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "test_gdds = [100, 300, 500, 800, 1000, 1200, 1500, 1800]\n",
    "for gdd in test_gdds:\n",
    "    stage, desc = predict_bbch(gdd)\n",
    "    print(f\"GDD = {gdd:4d} °C·days → BBCH {stage:2d} ({desc})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "residuals = y_test - best_model['predictions']\n",
    "\n",
    "# Residual distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(residuals, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Residual (BBCH units)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Residual Distribution')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(best_model['predictions'], residuals, alpha=0.5, color='steelblue')\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted BBCH Phase')\n",
    "ax2.set_ylabel('Residual')\n",
    "ax2.set_title('Residuals vs Predicted Values')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "ax3 = axes[2]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('residual_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.3f}\")\n",
    "print(f\"  Std:  {residuals.std():.3f}\")\n",
    "print(f\"  Skewness: {stats.skew(residuals):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for Random Forest)\n",
    "if 'Random Forest' in results:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    \n",
    "    # For single feature model, show prediction curve details\n",
    "    print(\"Random Forest Model Details:\")\n",
    "    print(f\"  Number of trees: {rf_model.n_estimators}\")\n",
    "    print(f\"  Max depth: {rf_model.max_depth}\")\n",
    "    print(f\"  Feature importance: GDD = 1.0 (single feature)\")\n",
    "    \n",
    "    # Show tree depth distribution\n",
    "    depths = [tree.get_depth() for tree in rf_model.estimators_]\n",
    "    print(f\"  Tree depth - Mean: {np.mean(depths):.1f}, Max: {max(depths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"WINTER WHEAT BBCH PREDICTION MODEL - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. MODEL PARAMETERS:\")\n",
    "print(f\"   - Base temperature (T_base): {PARAMS['T_base']}°C\")\n",
    "print(f\"   - Upper temperature cutoff: {PARAMS['T_upper']}°C\")\n",
    "print(f\"   - GDD calculation: Sum of (T_mean - T_base) for T_mean > T_base\")\n",
    "\n",
    "print(\"\\n2. DATASET:\")\n",
    "print(f\"   - Total samples: {len(training_data)}\")\n",
    "print(f\"   - BBCH stages modeled: {sorted(training_data['phase_id'].unique())}\")\n",
    "print(f\"   - Training/Test split: 80%/20%\")\n",
    "\n",
    "print(\"\\n3. MODEL PERFORMANCE:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   - MAE:  {results[best_model_name]['MAE']:.2f} BBCH units\")\n",
    "print(f\"   - RMSE: {results[best_model_name]['RMSE']:.2f} BBCH units\")\n",
    "print(f\"   - R²:   {results[best_model_name]['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n4. GDD THRESHOLDS (Mean ± Std):\")\n",
    "for phase_id, row in gdd_thresholds.iterrows():\n",
    "    phase_name = PARAMS['bbch_stages'].get(phase_id, f'Phase {phase_id}')\n",
    "    print(f\"   BBCH {phase_id:2d} ({phase_name:25s}): {row['Mean GDD']:6.0f} ± {row['Std GDD']:4.0f} °C·days\")\n",
    "\n",
    "print(\"\\n5. DATA SOURCES:\")\n",
    "print(\"   - DWD Climate Data Center (CDC)\")\n",
    "print(\"   - https://opendata.dwd.de/climate_environment/CDC/\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "\n",
    "model_data = {\n",
    "    'model': best_model_obj,\n",
    "    'model_name': best_model_name,\n",
    "    'parameters': PARAMS,\n",
    "    'gdd_thresholds': gdd_thresholds.to_dict(),\n",
    "    'metrics': {\n",
    "        'MAE': results[best_model_name]['MAE'],\n",
    "        'RMSE': results[best_model_name]['RMSE'],\n",
    "        'R2': results[best_model_name]['R2']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('bbch_prediction_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\"Model saved to 'bbch_prediction_model.pkl'\")\n",
    "print(\"\\nTo load and use the model:\")\n",
    "print(\"\")\n",
    "print(\"  import pickle\")\n",
    "print(\"  with open('bbch_prediction_model.pkl', 'rb') as f:\")\n",
    "print(\"      model_data = pickle.load(f)\")\n",
    "print(\"  \")\n",
    "print(\"  model = model_data['model']\")\n",
    "print(\"  prediction = model.predict([[500]])  # GDD = 500\")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}